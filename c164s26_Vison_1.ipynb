{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh+wU65rHrusj4iI6QlBX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/everestso/Summer24/blob/main/c164s26_Vison_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  AI Project Report  \n",
        "## Team: **Humanâ€“AI Collaborative Intelligence**\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Œ Project Title  \n",
        "**Exploring Intelligent Agents and Decision-Making in AI Systems**\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ‘¥ Team Members\n",
        "\n",
        "#### **David Ruby**  \n",
        "**Role:** Human Researcher & Project Lead  \n",
        "**Expertise:**  \n",
        "- Artificial Intelligence & Machine Learning  \n",
        "- Reinforcement Learning and Intelligent Agents  \n",
        "- Programming (Python, C++, SQL)  \n",
        "\n",
        "**Contributions:**  \n",
        "- Defined project goals and research questions  \n",
        "- Designed experiments and evaluation criteria  \n",
        "- Implemented core algorithms and analysis  \n",
        "- Interpreted results and ensured conceptual understanding  \n",
        "\n",
        "---\n",
        "\n",
        "#### **ChatGPT (AI Assistant)**  \n",
        "**Role:** AI Collaborator & Learning Support Tool  \n",
        "**Expertise:**  \n",
        "- Concept explanation and brainstorming  \n",
        "- Code generation, debugging, and refactoring  \n",
        "- Documentation and formatting assistance  \n",
        "\n",
        "**Contributions:**  \n",
        "- Assisted with idea generation and alternative approaches  \n",
        "- Provided explanations of AI concepts and algorithms  \n",
        "- Helped draft and refine code, comments, and written sections  \n",
        "- Supported clarity, organization, and presentation  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ¤ Collaboration Statement\n",
        "This project was completed through an intentional **humanâ€“AI collaboration**.  \n",
        "All AI-assisted content was critically evaluated, modified, and validated by the human team member.  \n",
        "The final submission reflects **David Rubyâ€™s understanding, reasoning, and responsibility** for the work.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸŽ“ Course\n",
        "**Undergraduate Artificial Intelligence**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sSbqYcThkw7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision w/ Hugging Face\n",
        "\n",
        "The Hugging Face **Transformers** library provides high-level pipelines for working with\n",
        "computer vision and multimodal models, including image captioning and prompted\n",
        "visionâ€“language generation. These pipelines abstract away much of the preprocessing,\n",
        "model loading, and inference logic, allowing you to experiment quickly with\n",
        "state-of-the-art models.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“˜ Core Documentation\n",
        "\n",
        "- **Vision & Multimodal Tasks (Overview)**  \n",
        "  https://huggingface.co/docs/transformers/tasks/vision\n",
        "\n",
        "- **Pipelines API (How `pipeline()` works)**  \n",
        "  https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "---\n",
        "\n",
        "## Common Image Captioning Models\n",
        "\n",
        "| Model | Promptable | Verbosity | Teaching Use |\n",
        "|------|------------|-----------|--------------|\n",
        "| ViT-GPT2 (`nlpconnect/vit-gpt2-image-captioning`) | âŒ | Lowâ€“Medium | Simple intro to image captioning |\n",
        "| BLIP (`Salesforce/blip-image-captioning-base/large`) | âœ… | Mediumâ€“High | Best all-around for multimodal prompting |\n",
        "| BLIP-2 (`Salesforce/blip2-flan-t5-*`) | âœ…âœ… | High | Advanced vision + instruction tuning |\n",
        "| GIT (`microsoft/git-base`) | âŒ | Medium | More literal, factual captions |\n",
        "| OFA (`OFA-Sys/ofa-base`) | âš ï¸ | Medium | Unified multimodal model concept |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— Model-Specific References (Optional Deep Dives)\n",
        "\n",
        "- **BLIP Models (Image Captioning & VQA)**  \n",
        "  https://huggingface.co/Salesforce/blip-image-captioning-base\n",
        "\n",
        "- **BLIP-2 (Vision + Instruction-Following LLMs)**  \n",
        "  https://huggingface.co/docs/transformers/model_doc/blip-2\n",
        "\n",
        "- **GIT (Generative Image-to-Text)**  \n",
        "  https://huggingface.co/microsoft/git-base\n",
        "\n",
        "- **OFA (Unified Multimodal Model)**  \n",
        "  https://huggingface.co/OFA-Sys/ofa-base\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** While these models are accessed using the `image-to-text` pipeline,\n",
        "> some (such as BLIP and BLIP-2) also accept text prompts that *condition* how the\n",
        "> image is interpreted and described. This makes them a useful stepping stone toward\n",
        "> more advanced multimodal and agentic AI systems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rdmqm4lWrtsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/pipeline_tutorial"
      ],
      "metadata": {
        "id": "Pr_ul6GiDGDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/file/d/1FNiCMMrxiIq00nnrdSM-HjE93o7EVFp3/view?usp=sharing\n",
        "\n"
      ],
      "metadata": {
        "id": "iPmNi5xNBfOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## !pip install -q gdown\n",
        "\n",
        "\n",
        "# Download the file from Google Drive\n",
        "# The file ID is extracted from the URL: https://drive.google.com/file/d/1FNiCMMrxiIq00nnrdSM-HjE93o7EVFp3/view?usp=sharing\n",
        "!gdown 1FNiCMMrxiIq00nnrdSM-HjE93o7EVFp3\n",
        "\n",
        "fn=\"PXL_20260128_185635132~2.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlH4Ry84BnNy",
        "outputId": "c63d8f35-9db4-488a-9010-e6a804911120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FNiCMMrxiIq00nnrdSM-HjE93o7EVFp3\n",
            "To: /content/PXL_20260128_185635132~2.jpg\n",
            "\r  0% 0.00/1.29M [00:00<?, ?B/s]\r100% 1.29M/1.29M [00:00<00:00, 13.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqo5AvrpGV8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity(40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxXbO_9pNUL",
        "outputId": "c428014d-c76c-4676-f490-b7b16a6e3a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Task: Image captioning\n",
        "captioner = pipeline(task=\"image-to-text\",\n",
        "                     model=\"nlpconnect/vit-gpt2-image-captioning\")"
      ],
      "metadata": {
        "id": "2MlXWPJOpRHi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captioner(fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BtNwLtUpUZv",
        "outputId": "2c5faf23-c111-4c42-d998-2224383aeee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'a bird is sitting on a piece of paper '}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompted Image Captioning\n",
        "\n",
        "Traditional image captioning models generate a short description of an image with no additional guidance. While useful, these captions are often brief and generic. **Prompted image captioning** extends this idea by allowing a *text prompt* to accompany the image, guiding how the model interprets and describes what it sees.\n",
        "\n",
        "In prompted image captioning, the model receives **both visual input and textual context** at the same time. The text prompt acts as an instruction, question, or stylistic guide that shapes the generated caption.\n",
        "\n",
        "For example, the same image may produce very different outputs depending on the prompt:\n",
        "\n",
        "- *â€œDescribe the image.â€*  \n",
        "- *â€œDescribe the image in vivid botanical detail.â€*  \n",
        "- *â€œWhat season does this image suggest?â€*  \n",
        "- *â€œList the visible objects and their colors.â€*\n",
        "\n",
        "Models such as **BLIP** and **BLIP-2** are explicitly designed to support this form of multimodal prompting. Internally, these models combine image embeddings with language model instructions, enabling them to:\n",
        "- Generate longer and more descriptive captions\n",
        "- Answer questions about an image\n",
        "- Adjust tone, detail level, or focus based on the prompt\n",
        "\n",
        "Prompted image captioning is an important step toward **multimodal reasoning systems**. Rather than passively describing images, models can be *steered* to interpret visual information in task-specific waysâ€”an ability that later becomes central in **agentic AI systems**, where perception is guided by goals and instructions.\n"
      ],
      "metadata": {
        "id": "0GV50CRnsZ2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#captioner2 = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
        "captioner2 = pipeline(\"image-to-text\", model=\"Salesforce/blip2-flan-t5-xl\")\n"
      ],
      "metadata": {
        "id": "TwMDuTyXsdsO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic caption\n",
        "print(captioner_blip2(\"tulips.jpg\"))\n",
        "\n",
        "# Prompted caption (steer style/detail)\n",
        "print(captioner_blip2(\"tulips.jpg\", prompt=\"Describe the image in vivid botanical detail:\"))\n",
        "\n",
        "# More verbose caption via generation params\n",
        "print(captioner_blip2(\"tulips.jpg\", prompt=\"Describe the image in detail:\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW8gHA6rr6qT",
        "outputId": "4b0b8f80-aa0b-4936-eb36-c43cf866d256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'tulips in bloom in a garden'}]\n",
            "[{'generated_text': 'a lily of the valley'}]\n",
            "[{'generated_text': 'The image shows a man in a red shirt and a woman in a red shirt.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic caption\n",
        "print(captioner_blip2(\"dog.jpg\"))\n",
        "\n",
        "# More verbose caption via generation params\n",
        "print(captioner_blip2(\"dog.jpg\", prompt=\"Describe the image in detail:\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnMJ8hRj1w-2",
        "outputId": "3c09b694-c5c9-458b-dcd3-6ce107798436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'a small white and brown dog is looking up'}]\n",
            "[{'generated_text': 'The image shows a man in a red shirt and a woman in a red shirt.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqa = pipeline(\n",
        "    \"visual-question-answering\",\n",
        "    model=\"Salesforce/blip2-flan-t5-xl\"\n",
        ")"
      ],
      "metadata": {
        "id": "bQsj6JM--jUM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vqa(image=\"dog.jpg\", question=\"Describe the dogâ€™s appearance and posture in detail.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eDn1avZ-wGT",
        "outputId": "53768c0e-7563-4d31-cda9-8415602ff8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'The dog is a stout, muscular dog with a slender body and a slender neck. The dog is a stout, muscular dog with a slender body and a slender neck.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "\n",
        "image = Image.open(\"dog.jpg\").convert(\"RGB\")\n",
        "\n",
        "inputs = processor(\n",
        "    image,\n",
        "    text=\"Describe the dogâ€™s appearance and posture in detail.\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=60\n",
        "    )\n",
        "\n",
        "print(processor.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "FCBTFucnAVCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}